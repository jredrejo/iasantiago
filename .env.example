TZ=Europe/Madrid

# ========= VLLM =========
VLLM_MODEL=mistralai/Mistral-7B-Instruct-v0.3-GPTQ
VLLM_GPU_MEMORY_UTILIZATION=0.7
VLLM_MAX_MODEL_LEN=4096
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_MAX_MODEL_LEN=32768           # 32k tokens (máximo del modelo)
VLLM_MAX_NUM_BATCHED_TOKENS=8192   # Aumentado para contextos largos
VLLM_MAX_TOKENS=16384              # ⭐ Tokens máximos de respuesta (50% del modelo)
VLLM_ATTENTION_BACKEND=FLASH_ATTN
VLLM_WORKER_MULTIPROC_METHOD=spawn
VLLM_LOGGING_LEVEL=INFO
VLLM_MAX_NUM_SEQS=64
VLLM_BLOCK_SIZE=16

# Optimizaciones para latencia
VLLM_MAX_NUM_SEQS=64            # Mantén esto alto para throughput
VLLM_MAX_NUM_BATCHED_TOKENS=4096 # Match con max_model_len
VLLM_BLOCK_SIZE=16               # Eficiencia de memory
VLLM_MAX_TOKENS=1024              # Tokens máximos de respuesta



# ============================================================
# SAMPLING PARAMETERS
# ============================================================
# For Q&A mode (factual, accurate)
RESPONSE_TEMPERATURE=0.3
RESPONSE_TOP_P=0.85

# For exam generation (structured, consistent)
GENERATIVE_TEMPERATURE=0.2
GENERATIVE_TOP_P=0.80



# ============================================================
# RAG - RETRIEVAL (cuántos chunks recuperar)
# ============================================================
FINAL_TOPK=12                       # Chunks base en modo respuest
GENERATIVE_TOPK_MULTIPLIER=4       # ⭐ En modo generativo: 5 × 4 = 20 chunks

MAX_CHUNKS_PER_FILE=8              # Máximo de chunks del mismo archivo
HYBRID_DENSE_K=40                  # K inicial para búsqueda densa
HYBRID_BM25_K=40                   # K inicial para búsqueda BM25
BM25_FALLBACK_TOKEN_THRESHOLD=4    # Usar solo BM25 si query < 4 tokens

# ============================================================
# GENERACIÓN - PORCENTAJES DE max_tokens (AJUSTADOS)
# ============================================================
GENERATIVE_MAX_TOKENS_PERCENT=70   # ⭐ AUMENTADO: En generativo usar hasta 70% del modelo (era 60%)
RESPONSE_MAX_TOKENS_PERCENT=25     # En respuesta: usar hasta 25% del modelo
MIN_RESPONSE_TOKENS=512            # Mínimo absoluto de tokens de respuesta

# ============================================================
# EMBEDDINGS Y RERANKING
# ============================================================
EMBED_MODEL_DEFAULT=intfloat/multilingual-e5-large-instruct
RERANK_MODEL=jinaai/jina-reranker-v2-base-multilingual
# Embedders por tema (opcional; vacío => usa default)
EMBED_MODEL_PROGRAMMING=thenlper/gte-large
EMBED_MODEL_ELECTRONICS=intfloat/multilingual-e5-large-instruct
EMBED_MODEL_CHEMISTRY=hkunlp/instructor-large

# Alternativa: BAAI/bge-reranker-large
# RERANK_MODEL=BAAI/bge-reranker-v2-m3
# ============================================================
# INFRAESTRUCTURA
# ============================================================
QDRANT_URL=http://qdrant:6333
BM25_BASE_DIR=/whoosh
TOPIC_BASE_DIR=/topics
UPSTREAM_OPENAI_URL=http://vllm:8000/v1
OPENAI_API_KEY=dummy-key
TELEMETRY_PATH=/app/retrieval.jsonl


CTX_TOKENS_SOFT_LIMIT=4000
CTX_TOKENS_GENERATIVE=10000

MAX_CHUNKS_PER_FILE=4
HYBRID_DENSE_K=40
HYBRID_BM25_K=40
FINAL_TOPK=8

# fallback a BM25 si consulta < 4 tokens
BM25_FALLBACK_TOKEN_THRESHOLD=4

# ============================================================
# TIMEOUTS
# ============================================================
VLLM_HEALTH_CHECK_TIMEOUT=15
VLLM_CONNECT_TIMEOUT=20
VLLM_STREAM_TIMEOUT=600

# === Qdrant ===
QDRANT_URL=http://qdrant:6333
QDRANT_STORAGE=/docker/iaburuaga-rag/data/storage

# === BM25 (Whoosh) ===
BM25_BASE_DIR=/docker/iaburuaga-rag/data/whoosh

# === Telemetría ===
TELEMETRY_PATH=/docker/iaburuaga-rag/rag-api/retrieval.jsonl

# ========= OPENAI-COMPAT / TOPIC SELECTOR =========
TOPIC_BASE_DIR=/docker/iaburuaga-rag/topics
# Etiquetas que verá Open WebUI en el desplegable de modelos
TOPIC_LABELS="Chemistry,Electronics,Programming,Sostenibilidad"
# Nombre base visto por Open WebUI
OPENAI_BASE_NAME="topic"

# Cuando OWUI llama /v1/models, exponemos:
#   topic:Chemistry / topic:Electronics / topic:Programming

# ========= SECURITY / AUTH =========
# OAuth2-Proxy
OAUTH2_CLIENT_ID=CHANGEME_GOOGLE_CLIENT_ID
OAUTH2_CLIENT_SECRET=CHANGEME_GOOGLE_CLIENT_SECRET
OAUTH2_REDIRECT_URL=https://ia.santiagoapostol.net/oauth2/callback
OAUTH2_EMAIL_DOMAINS=santiagoapostol.net


OAUTH2_PROXY_REDIRECT_URL=https://ia.santiagoapostol.net/oauth2/callback
OAUTH2_PROXY_WHITELIST_DOMAIN=.santiagoapostol.net
OAUTH2_PROXY_COOKIE_DOMAIN=.santiagoapostol.net
OAUTH2_PROXY_COOKIE_SECURE=true
OAUTH2_PROXY_COOKIE_SAMESITE=None
# Genera 32 bytes aleatorios en hex para OAUTH2_COOKIE_SECRET
# python -c "import secrets; print(secrets.token_hex(16))"
OAUTH2_COOKIE_SECRET=CALCULA_USANDO_EL_COMANDO_ANTERIOR

# ========= NGINX SSL (self-signed o reales si tenéis CA interna)
SSL_CERT_PATH=/etc/nginx/ssl/iasantiago.crt
SSL_KEY_PATH=/etc/nginx/ssl/iasantiago.key

# === Open WebUI ===
OPENWEBUI_PORT=8080

# ========= BACKUP =========
BACKUP_DIR=/opt/backups/rag-system


# === OpenAI compat ===
UPSTREAM_OPENAI_URL=http://vllm:8000/v1
OPENAI_API_KEY=dummy-key   # OpenWebUI requiere algo


# CUDA Configuration
# Para RTX 5090 (Blackwell): 10.0
# Para RTX 4090/4080/4070: 8.9
# Para RTX 3090/3080/3070: 8.6
# Para RTX 2080/2070: 7.5
# Para GTX 1080/1070: 6.1
# Si no estás seguro, deja en blanco o usa "native"
TORCH_CUDA_ARCH_LIST=native

# Embedding Configuration
# Opciones: "cuda" o "cpu"
# Si tienes problemas con GPU, usa "cpu"
EMBEDDING_DEVICE=cuda

# Dtype para embeddings
# Opciones: "float16" (ahorra memoria) o "float32" (más estable)
EMBEDDING_DTYPE=float32

# Debug CUDA (descomenta para debugging)
# CUDA_LAUNCH_BLOCKING=1
# TORCH_USE_CUDA_DSA=1


# docling
ENABLE_DOCLING=true
DOCLING_URL=http://docling-service:8003
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:False,max_split_size_mb:512
# PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024
CUDA_LAUNCH_BLOCKING=1
DOCLING_GPU_MEMORY_FRACTION=0.30
DOCLING_TABLE_MAX_BATCH_SIZE=4
