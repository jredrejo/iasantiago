services:
  qdrant:
    image: qdrant/qdrant:v1.15.5-gpu-nvidia
    container_name: qdrant
    restart: unless-stopped
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__GPU__INDEXING=1
      - TZ=${TZ}
    volumes:
      - ${QDRANT_STORAGE}:/qdrant/storage
    ports:
      - "6333:6333"
      - "6334:6334"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/readyz"]
      interval: 10s
      timeout: 3s
      retries: 30

  vllm:
  #  image: vllm/vllm-openai:latest
#    image: vllm/vllm-openai:v0.5.4
    image: vllm/vllm-openai:nightly
    container_name: vllm
    restart: unless-stopped
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    shm_size: 16gb
    env_file: .env
    environment:
    - TORCH_CUDA_ARCH_LIST=89  # RTX5090 es arquitectura 89
    - VLLM_TORCH_COMPILE_LEVEL=0
    - CUDA_LAUNCH_BLOCKING=0
    - TORCH_CUDA_MEMORY_FRACTION=0.85
    - CUDA_DEVICE_ORDER=PCI_BUS_ID
    - CUDA_VISIBLE_DEVICES=0
    - VLLM_LOGGING_LEVEL=INFO
    # Para latencia: enable prefill pipelining
    - VLLM_ENABLE_PREFIX_CACHING=1
    # Reduce overhead de comunicación
    - VLLM_ALLOW_LONG_MAX_POSITION_EMBEDDINGS=1
    ports:
      - "8000:8000"
    command: >
      --serve ${VLLM_MODEL}
      --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE}
      --max-model-len ${VLLM_MAX_MODEL_LEN}
      --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTILIZATION}
      --dtype bfloat16
      --host 0.0.0.0
      --port 8000
      --max-num-seqs ${VLLM_MAX_NUM_SEQS}
      --max-num-batched-tokens ${VLLM_MAX_NUM_BATCHED_TOKENS}
      --block-size ${VLLM_BLOCK_SIZE}
      --kv-cache-dtype auto
      --enable-prefix-caching
      --enable-chunked-prefill
    volumes:
      - huggingface_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 15s
      timeout: 5s
      retries: 30

  # ============================================================
  # vLLM-LLaVA para análisis de imágenes/tablas
  # Se pausa/arranca bajo demanda por ingestor/manage_gpu.sh
  # ============================================================
  vllm-llava:
    image: vllm/vllm-openai:nightly
#    image: vllm/vllm-openai:latest
    container_name: vllm-llava
    restart: no
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    shm_size: 16gb
    env_file: .env
    environment:
      - TORCH_CUDA_ARCH_LIST=89  # RTX5090 es arquitectura 89
      - VLLM_TORCH_COMPILE_LEVEL=0
      - CUDA_LAUNCH_BLOCKING=1
      - TORCH_CUDA_MEMORY_FRACTION=0.75
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_LOGGING_LEVEL=INFO
      # Para latencia: enable prefill pipelining
      - VLLM_ENABLE_PREFIX_CACHING=1
      # Reduce overhead de comunicación
      - VLLM_ALLOW_LONG_MAX_POSITION_EMBEDDINGS=1
    ports:
      - "8002:8000"
    expose:
      - "8000"
    command: >
      --serve llava-hf/llava-1.5-7b-hf
      --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE}
      --max-model-len 2048
      --gpu-memory-utilization 0.6
      --dtype bfloat16
      --host 0.0.0.0
      --port 8000
      --kv-cache-dtype auto
      --enable-prefix-caching
      --enable-chunked-prefill
      --quantization bitsandbytes
      --load-format bitsandbytes
      --max-num-seqs 128
      --max-num-batched-tokens 4096
      --block-size 16
    volumes:
      - huggingface_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 15s
      timeout: 5s
      retries: 30

  rag-api:
    build:
      context: ./rag-api
    container_name: rag-api
    restart: unless-stopped
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    shm_size: 8gb
    env_file: .env
    environment:
      - BM25_BASE_DIR=/whoosh
      - TOPIC_BASE_DIR=/topics
    volumes:
      - ${TOPIC_BASE_DIR}:/topics:ro
      - ${BM25_BASE_DIR}:/whoosh
      - huggingface_cache:/root/.cache/huggingface
      - ./rag-api:/app
      - ${QDRANT_STORAGE}:/qdrant_storage
    ports:
      - "8001:8001"
    depends_on:
      - vllm
      - qdrant
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  ingestor:
    build:
      context: ./ingestor
      dockerfile: Dockerfile
    container_name: ingestor
    restart: "no"
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    shm_size: 16gb
    env_file: .env
    environment:
      - HF_HOME=/models_cache
      # Conecta a vLLM-LLaVA cuando está corriendo
      - VLLM_URL=http://vllm-llava:8000
      - LLAVA_CACHE_DB=/llava_cache/llava_cache.db
      - TOPIC_BASE_DIR=/topics
      - BM25_BASE_DIR=/whoosh
      - MIN_IMAGE_WIDTH=100
      - MIN_IMAGE_HEIGHT=100
      - LLM_ANALYSIS_TEMPERATURE=0.3
      - TOKENIZERS_PARALLELISM=false
      - OMP_NUM_THREADS=1
    volumes:
      - models_cache:/models_cache
      - ${TOPIC_BASE_DIR}:/topics
      - ${BM25_BASE_DIR}:/whoosh
      - llava_cache:/llava_cache
      - ./ingestor:/app
    depends_on:
      - qdrant
      - vllm-llava
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    environment:
      - TZ=${TZ}
      - OPENAI_API_BASE_URL=http://rag-api:8001/v1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ENABLE_OLLAMA_API=false
      - OLLAMA_BASE_URL=""
      - WEBUI_NAME=IA del Santiago Apóstol
      - WEBUI_URL=https://ia.santiagoapostol.net
      - ENABLE_ARENA=false
      - ENABLE_RAG=false
      # Confiar en los headers que envia oauth2-proxy
      - WEBUI_AUTH_TRUSTED_EMAIL_HEADER=X-Forwarded-Email
      - WEBUI_AUTH_TRUSTED_NAME_HEADER=X-Forwarded-User
      - WEBUI_AUTH_COOKIE_AGE=604800  # 7 days in seconds
      - WEBUI_SESSION_COOKIE_AGE=604800  # 7 days in seconds
      # Cookies "sanas" detrás de reverse proxy
      - WEBUI_SESSION_COOKIE_SAME_SITE=lax
      - WEBUI_AUTH_COOKIE_SAME_SITE=lax
      - WEBUI_SESSION_COOKIE_SECURE=true
      - WEBUI_AUTH_COOKIE_SECURE=true
    ports:
      - "8080:8080"
    volumes:
      - ./openwebui/data:/app/backend/data
      - ./openwebui/custom/apple-touch-icon.png:/app/backend/open_webui/static/apple-touch-icon.png:ro
      - ./openwebui/custom/custom.css:/app/backend/open_webui/static/custom.css:ro
      - ./openwebui/custom/favicon-96x96.png:/app/backend/open_webui/static/favicon-96x96.png:ro
      - ./openwebui/custom/favicon.ico:/app/backend/open_webui/static/favicon.ico:ro
      - ./openwebui/custom/favicon.png:/app/backend/open_webui/static/favicon.png:ro
      - ./openwebui/custom/logo.png:/app/backend/open_webui/static/logo.png:ro
      - ./openwebui/custom/splash.png:/app/backend/open_webui/static/splash.png:ro
    depends_on:
      - rag-api

  oauth2-proxy:
    image: quay.io/oauth2-proxy/oauth2-proxy:v7.7.1
    container_name: oauth2-proxy
    restart: unless-stopped
    ports:
      - "4180:4180"
    volumes:
      - ./oauth2-proxy/oauth2-proxy.cfg:/etc/oauth2-proxy/oauth2-proxy.cfg:ro
      - ./oauth2-proxy/templates:/etc/oauth2-proxy/templates:ro
    environment:
      - TZ=${TZ}
    command:
      - --config=/etc/oauth2-proxy/oauth2-proxy.cfg
      - --cookie-secret=${OAUTH2_COOKIE_SECRET}
      - --client-id=${OAUTH2_CLIENT_ID}
      - --client-secret=${OAUTH2_CLIENT_SECRET}
      - --redirect-url=${OAUTH2_REDIRECT_URL}
      - --email-domain=${OAUTH2_EMAIL_DOMAINS}
      - --code-challenge-method=S256
      - --provider=oidc
      - --oidc-issuer-url=https://accounts.google.com
      - --http-address=0.0.0.0:4180
      - --upstream=http://openwebui:8080
      - --custom-templates-dir=/etc/oauth2-proxy/templates
      - --cookie-expire=168h
      - --cookie-refresh=0h
    depends_on:
      - openwebui

volumes:
  huggingface_cache:
  llava_cache:
  models_cache:
