services:
  qdrant:
    image: qdrant/qdrant:v1.15.5-gpu-nvidia
    container_name: qdrant
    restart: unless-stopped
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__GPU__INDEXING=1
      - TZ=${TZ}
    volumes:
      - ${QDRANT_STORAGE}:/qdrant/storage
    ports:
      - "6333:6333"
      - "6334:6334"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/readyz"]
      interval: 10s
      timeout: 3s
      retries: 30

  # docling-service removed - extraction now runs inside ingestor

  vllm:
    image: vllm/vllm-openai:nightly
    container_name: vllm
    restart: unless-stopped
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    shm_size: 16gb
    env_file: .env
    environment:
      - TORCH_CUDA_ARCH_LIST=89 # RTX5090 es arquitectura 89
      - VLLM_TORCH_COMPILE_LEVEL=0
      - CUDA_LAUNCH_BLOCKING=0
      # - TORCH_CUDA_MEMORY_FRACTION=0.85 QWEN
      - PYTORCH_ALLOC_CONF=max_split_size_mb:512
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_LOGGING_LEVEL=INFO
      # Para latencia: enable prefill pipelining
      - VLLM_ENABLE_PREFIX_CACHING=1
      # Reduce overhead de comunicación
      - VLLM_ALLOW_LONG_MAX_POSITION_EMBEDDINGS=1
      - HF_MODEL_NAME=${VLLM_MODEL}
      - VLLM_ATTENTION_BACKEND=${VLLM_ATTENTION_BACKEND}
      - VLLM_WORKER_MULTIPROC_METHOD=${VLLM_WORKER_MULTIPROC_METHOD}
      - VLLM_LOGGING_LEVEL=${VLLM_LOGGING_LEVEL}
    ports:
      - "8000:8000"
    command: >
      --model ${VLLM_MODEL}
      --quantization ${VLLM_QUANTIZATION}
      --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE}
      --max-model-len ${VLLM_MAX_MODEL_LEN}
      --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTILIZATION}
      --dtype ${VLLM_DTYPE}
      --host 0.0.0.0
      --port 8000
      --max-num-seqs ${VLLM_MAX_NUM_SEQS}
      --max-num-batched-tokens ${VLLM_MAX_NUM_BATCHED_TOKENS}
      --block-size ${VLLM_BLOCK_SIZE}
      --kv-cache-dtype ${VLLM_KV_CACHE_DTYPE}
      --enable-prefix-caching
      --enable-chunked-prefill
      --disable-log-requests
    volumes:
      - huggingface_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 15s
      timeout: 5s
      retries: 30

  rag-api:
    build:
      context: ./rag-api
    container_name: rag-api
    restart: unless-stopped
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    shm_size: 8gb
    env_file: .env
    environment:
      - BM25_BASE_DIR=/whoosh
      - TOPIC_BASE_DIR=/topics
    volumes:
      - ${TOPIC_BASE_DIR}:/topics:ro
      - ${BM25_BASE_DIR}:/whoosh
      - huggingface_cache:/root/.cache/huggingface
      - ./rag-api:/app
      - ${QDRANT_STORAGE}:/qdrant_storage
    ports:
      - "8001:8001"
    depends_on:
      - vllm
      - qdrant
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  ingestor:
    build:
      context: ./ingestor
      dockerfile: Dockerfile
    container_name: ingestor
    restart: on-failure  # Restart on crash (segfault), but not on clean exit
    # Healthcheck monitors for stuck containers (visible via docker ps)
    # Signal handler in main.py forces exit on SIGSEGV; if still stuck, use docker-autoheal
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import os, time; f='/tmp/ingestor_heartbeat'; exit(0 if not os.path.exists(f) or time.time() - float(open(f).readline()) < 300 else 1)\""]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    shm_size: 16gb
    env_file: .env
    environment:
      - HF_HOME=/models_cache
      - TORCH_HOME=/root/.cache/torch
      - TRANSFORMERS_CACHE=/models_cache/transformers
      - TOPIC_BASE_DIR=/topics
      - BM25_BASE_DIR=/whoosh
      - MIN_IMAGE_WIDTH=100
      - MIN_IMAGE_HEIGHT=100
      - LLM_ANALYSIS_TEMPERATURE=0.3
      - TOKENIZERS_PARALLELISM=true
      - OMP_NUM_THREADS=10
      - NUMBA_NUM_THREADS=10 # Para NumPy/numba
      - MKL_NUM_THREADS=10 # Para Intel MKL
      - OPENBLAS_NUM_THREADS=10 # Para OpenBLAS
      # Docling GPU settings (now running inside ingestor)
      - DOCLING_GPU_MEMORY_FRACTION=${DOCLING_GPU_MEMORY_FRACTION:-0.30}
      # CUDA configuration for Docling (matches vLLM settings for RTX 5090)
      - TORCH_CUDA_ARCH_LIST=89 # RTX5090 arquitectura 89
      - CUDA_LAUNCH_BLOCKING=0 # Ejecución async (no bloqueante)
      - PYTORCH_ALLOC_CONF=max_split_size_mb:512 # Reducir fragmentación de memoria
      - CUDA_DEVICE_ORDER=PCI_BUS_ID # Ordenamiento consistente de dispositivos
      - CUDA_VISIBLE_DEVICES=0 # Selección explícita de GPU
    volumes:
      - models_cache:/models_cache
      - ${TOPIC_BASE_DIR}:/topics
      - ${BM25_BASE_DIR}:/whoosh
      - ./ingestor:/app
      - /var/run/docker.sock:/var/run/docker.sock
      - docling_cache:/cache/docling  # Docling extraction cache
    depends_on:
      - qdrant
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  openwebui:
    image: ghcr.io/open-webui/open-webui:cuda
    container_name: openwebui
    restart: unless-stopped
    environment:
      - TZ=${TZ}
      - OPENAI_API_BASE_URL=http://rag-api:8001/v1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ENABLE_OLLAMA_API=false
      - OLLAMA_BASE_URL=""
      - WEBUI_NAME=IA del Santiago Apóstol
      - WEBUI_URL=https://ia.santiagoapostol.net
      - ENABLE_ARENA=false
      - ENABLE_RAG=false
      # Confiar en los headers que envia oauth2-proxy
      - WEBUI_AUTH_TRUSTED_EMAIL_HEADER=X-Forwarded-Email
      - WEBUI_AUTH_TRUSTED_NAME_HEADER=X-Forwarded-User
      - WEBUI_AUTH_COOKIE_AGE=604800 # 7 days in seconds
      - WEBUI_SESSION_COOKIE_AGE=604800 # 7 days in seconds
      # Cookies "sanas" detrás de reverse proxy
      - WEBUI_SESSION_COOKIE_SAME_SITE=lax
      - WEBUI_AUTH_COOKIE_SAME_SITE=lax
      - WEBUI_SESSION_COOKIE_SECURE=true
      - WEBUI_AUTH_COOKIE_SECURE=true
    ports:
      - "8080:8080"
    volumes:
      - ./openwebui/data:/app/backend/data
      - ./openwebui/custom/apple-touch-icon.png:/app/backend/open_webui/static/apple-touch-icon.png:ro
      - ./openwebui/custom/custom.css:/app/backend/open_webui/static/custom.css:ro
      - ./openwebui/custom/favicon-96x96.png:/app/backend/open_webui/static/favicon-96x96.png:ro
      - ./openwebui/custom/favicon.ico:/app/backend/open_webui/static/favicon.ico:ro
      - ./openwebui/custom/favicon.png:/app/backend/open_webui/static/favicon.png:ro
      - ./openwebui/custom/logo.png:/app/backend/open_webui/static/logo.png:ro
      - ./openwebui/custom/splash.png:/app/backend/open_webui/static/splash.png:ro
    depends_on:
      - rag-api

  oauth2-proxy:
    image: quay.io/oauth2-proxy/oauth2-proxy:v7.7.1
    container_name: oauth2-proxy
    restart: unless-stopped
    ports:
      - "4180:4180"
    volumes:
      - ./oauth2-proxy/oauth2-proxy.cfg:/etc/oauth2-proxy/oauth2-proxy.cfg:ro
      - ./oauth2-proxy/templates:/etc/oauth2-proxy/templates:ro
    environment:
      - TZ=${TZ}
    command:
      - --config=/etc/oauth2-proxy/oauth2-proxy.cfg
      - --cookie-secret=${OAUTH2_COOKIE_SECRET}
      - --client-id=${OAUTH2_CLIENT_ID}
      - --client-secret=${OAUTH2_CLIENT_SECRET}
      - --redirect-url=${OAUTH2_REDIRECT_URL}
      - --email-domain=${OAUTH2_EMAIL_DOMAINS}
      - --code-challenge-method=S256
      - --provider=oidc
      - --oidc-issuer-url=https://accounts.google.com
      - --http-address=0.0.0.0:4180
      - --upstream=http://openwebui:8080
      - --custom-templates-dir=/etc/oauth2-proxy/templates
      - --cookie-expire=168h
      - --cookie-refresh=0h
    depends_on:
      - openwebui

volumes:
  huggingface_cache:
  models_cache:
  docling_cache:
