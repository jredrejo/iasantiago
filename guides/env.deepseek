TZ=Europe/Madrid

# ============================================================
# vLLM - MODELO DE RAZONAMIENTO (DeepSeek-R1-14B)
# ============================================================
VLLM_MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
VLLM_QUANTIZATION=fp8
VLLM_DTYPE=bfloat16

# ============================================================
# vLLM - MEMORIA Y RENDIMIENTO
# ============================================================
VLLM_GPU_MEMORY_UTILIZATION=0.72
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_MAX_MODEL_LEN=14336           # 14k tokens (más que Qwen2.5-14B)
VLLM_MAX_NUM_BATCHED_TOKENS=8192
VLLM_MAX_TOKENS=8192
VLLM_ATTENTION_BACKEND=FLASHINFER
VLLM_WORKER_MULTIPROC_METHOD=spawn
VLLM_LOGGING_LEVEL=INFO

# Optimizaciones
VLLM_KV_CACHE_GPU_FRACTION=0.35
VLLM_SWAP_SPACE=16
VLLM_ENFORCE_EAGER=False
VLLM_MAX_NUM_SEQS=16
VLLM_BLOCK_SIZE=16
VLLM_KV_CACHE_DTYPE=auto
VLLM_ENABLE_CHUNKED_PREFILL=true
VLLM_ENABLE_PREFIX_CACHING=true
VLLM_NUM_SCHEDULER_STEPS=10

# ============================================================
# RAG - CONTEXTO EDUCATIVO
# ============================================================
CTX_TOKENS_SOFT_LIMIT=7000         # Más contexto (era 6000)
CTX_TOKENS_GENERATIVE=5000         # Más para generar (era 4000)

# ============================================================
# RAG - RETRIEVAL
# ============================================================
FINAL_TOPK=20                      # Más chunks
GENERATIVE_TOPK_MULTIPLIER=2
MAX_CHUNKS_PER_FILE=12
HYBRID_DENSE_K=80
HYBRID_BM25_K=80
BM25_FALLBACK_TOKEN_THRESHOLD=4

# ============================================================
# GENERACIÓN - TOKENS PARA RESPUESTA
# ============================================================
GENERATIVE_MAX_TOKENS_PERCENT=55   # 55% de 14k = 7.7k tokens
RESPONSE_MAX_TOKENS_PERCENT=45     # 45% de 14k = 6.3k tokens
MIN_RESPONSE_TOKENS=1024




# ============================================================

# ============================================================
# EMBEDDINGS Y RERANKING
# ============================================================
EMBED_MODEL_DEFAULT=intfloat/multilingual-e5-large-instruct
RERANK_MODEL=jinaai/jina-reranker-v2-base-multilingual
# Embedders por tema (opcional; vacío => usa default)
EMBED_MODEL_PROGRAMMING=thenlper/gte-large
EMBED_MODEL_CHEMISTRY=hkunlp/instructor-large
TORCH_BACKENDS_CUDNN_BENCHMARK=1
#PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,expandable_segments:True
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:256,roundup_power2_divisions:16
# Alternativa: BAAI/bge-reranker-large
# RERANK_MODEL=BAAI/bge-reranker-v2-m3
# ============================================================
# INFRAESTRUCTURA
# ============================================================
QDRANT_URL=http://qdrant:6333
BM25_BASE_DIR=/whoosh
TOPIC_BASE_DIR=/topics
UPSTREAM_OPENAI_URL=http://vllm:8000/v1
OPENAI_API_KEY=dummy-key
TELEMETRY_PATH=/app/retrieval.jsonl

# ============================================================
# TIMEOUTS
# ============================================================
VLLM_HEALTH_CHECK_TIMEOUT=15
VLLM_CONNECT_TIMEOUT=20
VLLM_STREAM_TIMEOUT=600

# === Qdrant ===
QDRANT_STORAGE=/opt/iasantiago-rag/data/storage

# === BM25 (Whoosh) ===
BM25_BASE_DIR=/opt/iasantiago-rag/data/whoosh

# === Telemetría ===
TELEMETRY_PATH=/opt/iasantiago-rag/rag-api/retrieval.jsonl

# ========= OPENAI-COMPAT / TOPIC SELECTOR =========
TOPIC_BASE_DIR=/opt/iasantiago-rag/topics
# Etiquetas que verá Open WebUI en el desplegable de modelos
TOPIC_LABELS="AFD,Chemistry,Dibujo,Electricidad,FOL,Latin,Mecanica,Programming,Sostenibilidad"
# Nombre base visto por Open WebUI
OPENAI_BASE_NAME="topic"

# Cuando OWUI llama /v1/models, exponemos:
#   topic:Chemistry / topic:Electronics / topic:Programming

# ========= SECURITY / AUTH =========
# OAuth2-Proxy
OAUTH2_CLIENT_ID=CHANGEME_GOOGLE_CLIENT_ID
OAUTH2_CLIENT_SECRET=CHANGEME_GOOGLE_CLIENT_SECRET
OAUTH2_REDIRECT_URL=https://ia.santiagoapostol.net/oauth2/callback
OAUTH2_EMAIL_DOMAINS=santiagoapostol.net

GOOGLE_DRIVE_API_KEY=CHANGEME_GOOGLE_DRIVE_API_KEY
OAUTH2_PROXY_REDIRECT_URL=https://ia.santiagoapostol.net/oauth2/callback
OAUTH2_PROXY_WHITELIST_DOMAIN=.santiagoapostol.net
OAUTH2_PROXY_COOKIE_DOMAIN=.santiagoapostol.net
OAUTH2_PROXY_COOKIE_SECURE=true
OAUTH2_PROXY_COOKIE_SAMESITE=None
# Genera 32 bytes aleatorios en hex para OAUTH2_COOKIE_SECRET
# python -c "import secrets; print(secrets.token_hex(16))"
OAUTH2_COOKIE_SECRET=CALCULA_USANDO_EL_COMANDO_ANTERIOR

# ========= NGINX SSL (self-signed o reales si tenéis CA interna)
SSL_CERT_PATH=/etc/nginx/ssl/iasantiago.crt
SSL_KEY_PATH=/etc/nginx/ssl/iasantiago.key

# === Open WebUI ===
OPENWEBUI_PORT=8080

# ========= BACKUP =========
BACKUP_DIR=/opt/backups/rag-system


# === OpenAI compat ===
UPSTREAM_OPENAI_URL=http://vllm:8000/v1
OPENAI_API_KEY=dummy-key   # OpenWebUI requiere algo


# CUDA Configuration
# Para RTX 5090 (Blackwell): 10.0
# Para RTX 4090/4080/4070: 8.9
# Para RTX 3090/3080/3070: 8.6
# Para RTX 2080/2070: 7.5
# Para GTX 1080/1070: 6.1
# Si no estás seguro, deja en blanco o usa "native"
TORCH_CUDA_ARCH_LIST=native

# Embedding Configuration
# Opciones: "cuda" o "cpu"
# Si tienes problemas con GPU, usa "cpu"
EMBEDDING_DEVICE=cuda

# Dtype para embeddings
# Opciones: "float16" (ahorra memoria) o "float32" (más estable)
EMBEDDING_DTYPE=float32

# Debug CUDA (descomenta para debugging)
# CUDA_LAUNCH_BLOCKING=1
# TORCH_USE_CUDA_DSA=1
RAG_ALLOWED_FILE_EXTENSIONS=pdf,txt,doc,docx,xls,xlsx,ppt,pptx,md,csv,json

UNSTRUCTURED_LANGUAGES=spa,eng
UNSTRUCTURED_FALLBACK_LANGUAGE=eng
USE_GPU_OCR=true
GPU_OCR_MEMORY_LIMIT=2048

# docling
ENABLE_DOCLING=true
DOCLING_URL=http://docling-service:8003
# PYTORCH_CUDA_ALLOC_CONF=expandable_segments:False,max_split_size_mb:512
# PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024
CUDA_LAUNCH_BLOCKING=1
DOCLING_GPU_MEMORY_FRACTION=0.30
DOCLING_TABLE_MAX_BATCH_SIZE=4


PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:256
CUDA_LAUNCH_BLOCKING=1
